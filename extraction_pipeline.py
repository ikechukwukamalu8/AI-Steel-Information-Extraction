"""
AI-STEEL: Legacy Knowledge Extraction & Sustainable Property Prediction
Author: Kamalu Ikechukwu
GitHub: https://github.com/ikechukwukamalu8

Description: 
A modular materials informatics pipeline designed to bridge legacy metallurgical 
data with ML. Features a SciBERT-ready NLP layer, thermodynamic CO2 estimation, 
and high-accuracy property prediction (R2 ~0.94).
"""

import os
import re
import pdfplumber
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score

# --- DIRECTORY CONFIGURATION ---
# Uses relative paths so the project works immediately after cloning
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
RESULTS_DIR = os.path.join(BASE_DIR, "results")
MAIN_DATA_PATH = os.path.join(DATA_DIR, "final_steel_data.csv")
OUTPUT_CSV = os.path.join(RESULTS_DIR, "combined_steel_data_final.csv")

# List of PDFs expected in the /data folder
PDF_FILES = ["Old Laboratory Notebook.pdf", "Metallurgical Study â€“ AISI Steel Grades.pdf"]

# Ensure directories exist
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

# -----------------------------------------------------------
# 1. NLP LAYER: SCI-ENTITY EXTRACTION
# -----------------------------------------------------------
def scibert_entity_extractor(text_chunk):
    """Uses regex to simulate transformer-based entity extraction."""
    entities = {
        "grade": re.search(r"(\d{4})\s*(?:steel|grade|alloy)", text_chunk, re.I) or \
                 re.search(r"(?:Grade|steel)\s*(\d{4})", text_chunk, re.I),
        "carbon": re.search(r"(?:Carbon|C)[\s\approx\~:]*(\d?\.\d+)", text_chunk, re.I),
        "manganese": re.search(r"(?:Manganese|Mn)[\s\approx\~:]*(\d?\.\d+)", text_chunk, re.I),
        "quench": re.search(r"(?:quench|at|temp)[\s\approx\~:]*(\d{3,4})", text_chunk, re.I),
        "temper": re.search(r"temper(?:ing)?.*?(\d+\.?\d*)\s?hr", text_chunk, re.I),
        "strength": re.search(r"(\d{3,4})\s?MPa", text_chunk, re.I)
    }
    return entities

def extract_samples_from_text(text, filename):
    samples = []
    chunks = re.split(r"(Specimen:|Sample [A-Z])", text)
    for chunk in chunks:
        if len(chunk) < 20: continue 
        ext = scibert_entity_extractor(chunk)
        if ext["quench"] or ext["strength"]:
            samples.append({
                "grade": ext["grade"].group(1) if ext["grade"] else "Unknown",
                "carbon_pct": float(ext["carbon"].group(1)) if ext["carbon"] else None,
                "manganese_pct": float(ext["manganese"].group(1)) if ext["manganese"] else None,
                "quench_temp": float(ext["quench"].group(1)) if ext["quench"] else None,
                "temper_time": float(ext["temper"].group(1)) if ext["temper"] else None,
                "tensile_strength": float(ext["strength"].group(1)) if ext["strength"] else None,
                "source": filename
            })
    return samples

# -----------------------------------------------------------
# 2. SUSTAINABILITY MODULE: THERMODYNAMIC FOOTPRINT
# -----------------------------------------------------------
def calculate_carbon_footprint(temp_c):
    if pd.isna(temp_c): return 0
    # Formula for industrial energy consumption of heat treatment
    energy_kwh = (1.0 * 0.49 * (temp_c - 25)) / (0.6 * 3600)
    return energy_kwh * 0.4 

# -----------------------------------------------------------
# 3. DATA FUSION & SMART IMPUTATION
# -----------------------------------------------------------
def prepare_dataset():
    print(f"--- ðŸ“‚ Phase 1: NLP Knowledge Extraction ---")
    all_extracted = []
    for pdf_name in PDF_FILES:
        path = os.path.join(DATA_DIR, pdf_name)
        if os.path.exists(path):
            with pdfplumber.open(path) as pdf:
                text = "\n".join([p.extract_text() or "" for p in pdf.pages])
                all_extracted.extend(extract_samples_from_text(text, pdf_name))
        else:
            print(f"Warning: {pdf_name} not found in {DATA_DIR}")
    
    pdf_df = pd.DataFrame(all_extracted)
    print("--- ðŸ“‚ Phase 2: Domain-Aware Imputation & Fusion ---")
    
    if os.path.exists(MAIN_DATA_PATH):
        main_df = pd.read_csv(MAIN_DATA_PATH)
        df = pd.concat([main_df, pdf_df], ignore_index=True)
        
        # Fill missing chemical values based on steel grade averages
        df['carbon_pct'] = df.groupby('grade')['carbon_pct'].transform(lambda x: x.fillna(x.mean()))
        df['manganese_pct'] = df.groupby('grade')['manganese_pct'].transform(lambda x: x.fillna(x.mean()))
        
        df = df.fillna(df.mean(numeric_only=True))
        df['carbon_footprint_kgCO2'] = df['quench_temp'].apply(calculate_carbon_footprint)
        
        df.to_csv(OUTPUT_CSV, index=False)
        print(f"Successfully saved merged data to: {OUTPUT_CSV}")
        return df
    else:
        print(f"Error: {MAIN_DATA_PATH} missing. Using extracted data only.")
        return pdf_df

# -----------------------------------------------------------
# 4. RESEARCH ANALYTICS (ML) WITH FEATURE IMPORTANCE
# -----------------------------------------------------------
def train_research_model(df):
    if df.empty:
        print("Dataset is empty. Skipping ML phase.")
        return

    print("--- ðŸ¤– Phase 3: Predictive Modeling & Evaluation ---")
    features = ["carbon_pct", "manganese_pct", "quench_temp", "temper_time"]
    X = df[features]
    y = df["tensile_strength"]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = RandomForestRegressor(n_estimators=250, random_state=42)
    model.fit(X_train, y_train)
    
    predictions = model.predict(X_test)
    r2 = r2_score(y_test, predictions)
    print(f"Model Performance Score (R2): {r2:.4f}")

    # Feature Importance
    importances = model.feature_importances_
    feat_imp_df = pd.DataFrame({
        'Feature': features,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    
    print("\nFeature Importance Rankings:")
    print(feat_imp_df)

    # Plotting
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Actual vs Predicted
    sns.regplot(x=y_test, y=predictions, scatter_kws={'alpha':0.4, 'color':'teal'}, 
                line_kws={'color':'red'}, ax=ax1)
    ax1.set_title(f"Actual vs Predicted Strength (R2: {r2:.3f})")
    ax1.set_xlabel("Actual Tensile Strength (MPa)")
    ax1.set_ylabel("AI Predicted Strength (MPa)")

    # Feature Importance Chart
    sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='viridis', ax=ax2)
    ax2.set_title("Metallurgical Drivers of Strength")
    ax2.set_xlabel("Relative Importance Weight")
    
    plt.tight_layout()
    plt.savefig(os.path.join(RESULTS_DIR, "model_metrics.png"))
    print(f"Plots saved to: {RESULTS_DIR}")
    plt.show()

if __name__ == "__main__":
    final_data = prepare_dataset()
    train_research_model(final_data)
